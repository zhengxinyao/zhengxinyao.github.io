<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Xinyao</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="asplos24fall Paper #62 Reviews and Comments &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;">
<meta property="og:type" content="article">
<meta property="og:title" content="Xinyao">
<meta property="og:url" content="https://zhengxinyao.github.io/2024/02/07/About-me/index.html">
<meta property="og:site_name" content="Xinyao">
<meta property="og:description" content="asplos24fall Paper #62 Reviews and Comments &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="e:%5CA-be%20a%20phd%5Clibrary%5Cimage%5Cwps3.jpg">
<meta property="og:image" content="e:%5CA-be%20a%20phd%5Clibrary%5Cimage%5Cwps4.jpg">
<meta property="article:published_time" content="2024-02-07T15:01:19.025Z">
<meta property="article:modified_time" content="2024-02-14T08:03:27.663Z">
<meta property="article:author" content="ICT-IPRC">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="e:%5CA-be%20a%20phd%5Clibrary%5Cimage%5Cwps3.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Xinyao" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Xinyao</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zhengxinyao.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-About-me" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/02/07/About-me/" class="article-date">
  <time class="dt-published" datetime="2024-02-07T15:01:19.025Z" itemprop="datePublished">2024-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>asplos24fall Paper #62 Reviews and Comments</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<p>Paper #62 TensorTEE: Unified Granularity Heterogeneous TEE for Efficient</p>
<p>Secure Collaborative Computing</p>
<p>Review #62A</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<p>Overall merit</p>
<p>-————</p>
<p>\2. Weak reject</p>
<p>Reviewer expertise</p>
<p>-—————–</p>
<p>\2. Some familiarity</p>
<p>Paper summary</p>
<p>-————</p>
<p>TensorTEE is a unified trusted execution environment for CPU and NPU to enable secure LLM training. TensorTEE uses a unified granularity for the protection mechanisms for data in memory based on the tensor granularity to alleviate some of the challenges in current platforms. To accomplish this change TensorTEE includes an interface in both the NPU and CPU sides to translate cache line granularity memory accesses into tensor granular accesses that synchronize the security metadata.</p>
<p>Strengths</p>
<p>-——–</p>
<p>Secure heterogeneous computing system are an important research direction. </p>
<p>Interesting observation about the granularity incompatibility between CPU and NPU TEEs.</p>
<p>Weaknesses</p>
<p>-———</p>
<p>Limited contributions - delayed MAC verification has been done before.  </p>
<p>Unclear technical descriptions.</p>
<p>Some design choices break the threat model (such as the delayed integrity verification).</p>
<p>Detailed feedback</p>
<p>-—————-</p>
<p>Thank you for submitting your work to ASPLOS. While i appreciate the effort to improve security in LLM training, I am unclear of what exactly TensorTEE is. It may be due to the unclear technical description but as I understand it, TensorTEE simply translates CPU memory accesses into tensor granular accesses synchronizing the metadata for the corresponding data. It is unclear exactly how the synchronization happens, is it just assumed that all cache lines take on the same VN? Is the VN a max or a sum of all the VN? It is not clear how this is done. And in any case it is also not clear how the security of the system is impacted by this change. Changing the granularity of the security metadata makes it so that either more hash collisions are possible and&#x2F;or more VN overflows will exists. Your manuscript does not go over either of these scenarios. </p>
<p>Furthermore, the assumption that you can just create a secure channel between the two devices obviates the need for a system like TensorTEE as one could just imagine the CPU transferring the unprotected data over to the NPU over this secure channel and then just simply leaving it up to the NPU to manage its own security metadata. Something like Graviton with an added secure channel and without the need to encrypt the data. </p>
<p>You claim that one of the challenges of having the MAC be tensor granular is larger delays to verify the data. It is unclear to me why this is the case as usually MAC engines are pipelined and the throughput is the same, regardless of the length of the data. </p>
<p>Delayed MAC verification is not a novel concept and it has been shown to break the threat model. Delayed MAC verification has been proposed in the very early work of secure memory. However, it has been shown that it is not safe to continue operating on values before verification is completed as an attacker could leverage this speculative window to modify other components outside of the trusted boundary with unverified values (see below citations). How are you addressing this type of vulnerability in delayed MAC verification?  </p>
<p>More questions specific to the manuscript:</p>
<ol>
<li><p>in figure 7 what is on the y-axis? this figure is unclear to me. </p>
</li>
<li><p>In section 4.2 in the dataflow description there is a sentence that indicates that the VN may not be correct. Why is this? This detail is not clearly explained. </p>
</li>
<li><p>In section 4.4.2 under the Communication Phase paragraph it says “data corresponding to the address range is directly transferred from the CPU DRAM to the NPU’s DRAM via a direct channel, without involving the CPU and NPU.” How is the data transferred from CPU DRAM to NPU DRAM? Is it over a secure channel? Is a secure channel necessary? How is a man-in-the-middle attack prevented here?</p>
</li>
</ol>
<p>Citations:</p>
<p>[1] G. E. Suh, D. Clarke, B. Gassend, M. Van Dijk, and S. Devadas, “AEGIS: architecture for tamper-evident and tamper-resistant processing,”  International Conference on Supercomputing (ICS) 2003.</p>
<p>[2] W. Shi and H.-H. S. Lee, “Authentication control point and its implications for secure processor design,” International Symposium on Microarchitecture (MICRO) 2006.</p>
<p>[3] B. Rogers, S. Chhabra, M. Prvulovic, and Y. Solihin, “Using address independent seed encryption and bonsai merkle trees to make secure processors os-and performance-friendly,” International Symposium on Microarchitecture (MICRO) 2007.</p>
<p>[4] T.S. Lehman, A.D. Hilton, and B.C. Lee. “PoisonIvy: Safe speculation for secure memory.” International Symposium on Microarchitecture (MICRO) 2016.</p>
<p>Review #62B</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<p>Overall merit</p>
<p>-————</p>
<p>\3. Weak accept</p>
<p>Reviewer expertise</p>
<p>-—————–</p>
<p>\3. Knowledgeable</p>
<p>Paper summary</p>
<p>-————</p>
<p>The paper proposes an approach for a discrete, heterogeneous TEE which consists of a CPU and a NPU. The approach is evaluated using a cycle-accurate simulator.</p>
<p>Strengths</p>
<p>-——–</p>
<p>+ The paper is very well motivated and the approach is nicely introduced. </p>
<p>+ The approach makes sense and performance evaluation shows very limited overhead using concepts like delayed verification of tensor-granularity MACs </p>
<p>+ Looks like an approach that might be practical</p>
<p>Weaknesses</p>
<p>-———</p>
<p>+ Missing discussion of potential security impacts of the delayed verification</p>
<p>+ Missing impact on flexibility and software by using tensor granularity MACs and VNs</p>
<p>Detailed feedback</p>
<p>-—————-</p>
<p>The paper is well-written and has quite convincing arguments and evaluations. </p>
<p>Still, I wonder about the potential disadvantages of having tensor-granularity MACs and VNs. Does this potentially affect the software that runs on the CPU and how? </p>
<p>Is there a potential for side-channel attacks to have a delayed verification of MACs - since data is used before being verified? If not, why not? </p>
<p>How would this affect non-tensor-based computations on the CPU? Would the performance be affected? Would that software potentially also benefit from a larger VN granularity?</p>
<p>Review #62C</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<p>Overall merit</p>
<p>-————</p>
<p>\2. Weak reject</p>
<p>Reviewer expertise</p>
<p>-—————–</p>
<p>\3. Knowledgeable</p>
<p>Paper summary</p>
<p>-————</p>
<p>Background:</p>
<p>SGX uses version numbers alongside physical addresses to encrypt memory, and it also uses</p>
<p>Merkle trees to authenticate memory contents.</p>
<p>Assuming that secure LLM training on a CPU + NPU machine requires both to</p>
<p>use SGX-like memory protection, this submission introduces three optimizations</p>
<p>to reduce the overhead of memory protection:</p>
<ol>
<li>CPU memory controller extension that identifies tensors based on core memory</li>
</ol>
<p>accesses and uses a single version number for the entire tensor (MBs);</p>
<ol start="2">
<li><p>lazy tensor MAC verification that reverts computation if MAC verification fails; and</p>
</li>
<li><p>direct NPU access to CPU memory by sharing the same key, version numbers, and tensor encryption.</p>
</li>
</ol>
<p>All of the above, called TensorTEE, are verified using Gem5 simulations that show</p>
<p>up to 6x batch latency reduction for LLM training.</p>
<p>Strengths</p>
<p>-——–</p>
<p>+ Using the same key on both NPU and CPU is a neat idea</p>
<p>Weaknesses</p>
<p>-———</p>
<p>- Worst case scenarios are dismissed too quickly</p>
<p>- Dismisses simpler solutions too quickly</p>
<p>- Applicable only to SGX</p>
<p>Detailed feedback</p>
<p>-—————-</p>
<p>Thank you for submitting your work to ASPLOS’24! I enjoyed reading about</p>
<p>collaborative LLM training using TEEs. I think that aligning tensors between</p>
<p>CPU and NPU makes a lot of sense and that the benefit of using the same</p>
<p>key on both is clear. However, I’m less excited by speculatively identifying</p>
<p>tensors on the memory controller and lazily verifying tensor MAC. I think that</p>
<p>both of these can lead to significant degradation in the worst case, which</p>
<p>unfortunately is not evaluated. Also, I’m disappointed that simpler solutions,</p>
<p>such as not encrypting NPU memory or adding AES engines are dismissed off-hand</p>
<p>despite their prevalence with NVIDIA’s GPUs. Similarly, today’s GPUs support</p>
<p>confidential computing using VMs which side-step the problem tackled by this</p>
<p>paper, and I think that the authors should present the scope of their work more</p>
<p>clearly: applies only to SGX, and make an argument for the pros&#x2F;cons of NPU TEEs</p>
<p>based on this approach. See below in more details:</p>
<p>###  Worst case scenarios are dismissed too quickly</p>
<p>Two worst case scenarios should be discussed and evaluated:</p>
<p>(1) failure to detect a tensor; and</p>
<p>(2) lazy MAC verification failure.</p>
<p>Failure to detect a tensor can happen for several reasons,</p>
<p>including for example context switches on the CPU where</p>
<p>a different process will use the same virtual addresses</p>
<p>as the TensorTEE but this time with a different data structure.</p>
<p>In general, I’m skeptical that the memory controller can</p>
<p>successfully identify CPU data structure without failure.</p>
<p>Lazy MAC verification is considered safe because a rollback mechanism</p>
<p>exists. But, by the time MAC verification is complete, I’m not sure that a</p>
<p>rollback is possible without software involvement. A tensor can be as large as</p>
<p>several MB and as computation continues without verifying this data the CPU</p>
<p>won’t be able to save all this “speculative” state and revert it.</p>
<p>### Dismisses simpler solutions too quickly</p>
<ol>
<li>Is there a need to encrypt NPU memory?</li>
</ol>
<p>It appears that this submission rests on the need to encrypt NPU memory. But</p>
<p>what evidence is there to suggest that NPU memory is unsafe and can be accessed</p>
<p>by a malicious hypervisor or cloud service provider? To make this more specific,</p>
<p>[NVIDIA’s technical blog on confidential computing</p>
<p>explains](<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/">https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/</a>)</p>
<p>that: “on-package HBM memory is considered secure against everyday physical</p>
<p>attack tools and is not encrypted”. Is NVIDIA wrong?</p>
<p>Don’t get me wrong, I see that you are not the first to submit a paper in this</p>
<p>space, but I couldn’t find any concrete attacks suggesting that this memory</p>
<p>needs protection and I’ve looked in the references that the authors have</p>
<p>provided.</p>
<p>\1. 内存保护的文章认为on-pakage HBM是安全的，主要针对GDDR等DRAM内存设备。</p>
<p>GPU类例如“Common-counter”和“PSSM”，认为HBM不易受到物理攻击，但是HBM昂贵且目前市场规模较小，很多商用GPU仍然使用GDDRx的内存或者和CPU共享DDR内存，所以GPU TEE的内存保护依旧有意义，需要内存加密保护。MGX，TNPU等进行内存保护也考虑的是DRAM。</p>
<p>“However, Graviton assumes that 3D stacked DRAM such as HBM (High Bandwidth Memory) is available for GPUs, and the data in the 3D stacked DRAM is always safe even from any possible physical attacks. However, many commodity GPUs will still be relying on much less expensive GDDRx memory [33]–[35], vulnerable from physical attacks. In addition, embedded GPUs share the same DDRx memory with CPUs [3], [9], [10]. Considering the wide-spread use of GDDRx and its vulnerability from physical attacks, it is essential to investigate the performance implication of hardware-based memory protection for GPUs.”【Na S, Lee S, Kim Y, et al. Common counters: Compressed encryption counters for secure GPU memory[C]&#x2F;&#x2F;2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2021: 1-13.】</p>
<p>“As a result, the device memory is vulnerable to attacks, which may include eavesdropping or tampering of data in memory. Examples include passive eavesdropping between GPU and its device memory [ 32 ], cold boot attacks [ 11 ], rowhammer attacks [21], etc. ”</p>
<p>“Graviton assumes that the GPU device memory is soldered within the GPU package and thus assumes physical attacks on device memory are out of reach. However, due to the high cost of 3D stacked memory, discrete GDDR memory remains to be widely used in commercial GPUs, including the latest Nvidia Am- pere GPUs [2]. Discrete GDDR memory chips are readily accessible as it is a common practice to replace faulty GDDR-memory chips. Thus, GPU device memory will still be vulnerable to physical at- tacks. Moreover, with the new attack scheme like rowhammer [ 21 ]”【Yuan S, Solihin Y, Zhou H. Pssm: Achieving secure memory for gpus with partitioned and sectored security metadata[C]&#x2F;&#x2F;Proceedings of the ACM International Conference on Supercomputing. 2021: 139-151.】</p>
<p>“This paper assumes that GPUs’device memory, specifically, the GDDR memory, is also vulnerable to the physical attacks considered in CPU TEEs. The reason is that GDDR memory is off-chip for GPUs and can be fully exposed to attackers with physical access to the device. Thus, we exclude the GPU GDDR memory modules from the TCB, and assume that the GPU chip forms the security boundary. High bandwidth memory (HBM), however, is not vulnerable to physical attacks if it is soldered within the GPU chip package, and is out of the reach for attackers.”【Yuan S, Awad A, Yudha A W B, et al. Adaptive security support for heterogeneous memory on gpus[C]&#x2F;&#x2F;2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022: 213-228.】</p>
<p>有的文章假设GPU TEE和CPU TEE有一样的内存安全问题，但没有给出假设依据。</p>
<p>“Due to the enormity of the overheads, recent attempts to extend CPU TEE to GPUs [8], [29] ignore the physical attack threats and enlarge the trust base (e.g., adding GPU memory module to the trust base). In contrast, this research assumes the same threat model in CPU TEE also affects GPUs, and assumes that GPU chip provides the security boundary, where all the data stored in the on-chip resources such as registers and caches are safe. The attackers may have physical access to the GPUs hardware, and have the capability to snoop the GPU memory buses or to scan&#x2F;tamper the GPU memory content.”【Yuan S, Yudha A W B, Solihin Y, et al. Analyzing secure memory architecture for gpus[C]&#x2F;&#x2F;2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, 2021: 59-69.】</p>
<p>\2. HBM延迟高、带宽大，主要用于GPU、TPU等AI设备，但是HBM成本高扩展难需要封装，目前来看传统的DDR、LPDDR、GDDR以及HBM处于共存的阶段，比如NVIDIA的Grace Hopper 超级芯片。传统的DDR、LPDDR、GDDR以及HBM处于共存的阶段，比如NVIDIA的Grace Hopper 超级芯片，CPU用DDR，GPU用HBM。</p>
<p><img src="E:%5CA-be%20a%20phd%5Clibrary%5Cimage%5Cwps3.jpg" alt="img"> </p>
<p>2017年，AlphaGo再战柯洁，芯片换成了Google自家研发的TPU。在芯片设计上，从第二代开始的每一代TPU，都采用了HBM。英伟达针对数据中心和深度学习的新款GPU Tesla P100，也搭载了第二代HBM内存（HBM2）。高性能计算市场的GPU芯片几乎都配备了HBM内存。</p>
<p>生成式人工智能实现了爆发式发展，国内外大厂争相竞逐AI大模型，大模型训练的过程数据吞吐量很大，HBM成为了AI训练芯片的标配。目前主流的大模型训练芯片A100、H100均应用了HBM，与H100直接竞争的[谷歌TPU v5](<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E8%B0%B7%E6%AD%8CTPU">https://www.zhihu.com/search?q=谷歌TPU</a> v5&amp;search_source&#x3D;Entity&amp;hybrid_search_source&#x3D;Entity&amp;hybrid_search_extra&#x3D;{:,:3334832220})和AMD MI300即将量产，后两者同样将采用HBM，AWS的Trainium和Inferentia也应用了HBM，并且，Google与AWS正着手研发新一代AI加速芯片，将采用HBM3或HBM3e。</p>
<p><img src="E:%5CA-be%20a%20phd%5Clibrary%5Cimage%5Cwps4.jpg" alt="img"> </p>
<ol start="2">
<li>Arguments against adding more AES engines are weak</li>
</ol>
<p>Now, suppose the need to encrypt was clear, then why not add hardware to</p>
<p>perform this encryption? The CPU already has such hardware, so it should be</p>
<p>possible. The authors argue that this is not the case because:</p>
<p>&gt; The limited AES bandwidth that 8GB&#x2F;s on average […]</p>
<p>&gt; Given the scarcity of resources on the microprocessor die, the integration of</p>
<p>&gt; additional encryption engines (over 1 $mm^2$ per AES unit) into memory</p>
<p>&gt; controllers on the NPU dies poses a prohibitively high cost [17, 66].</p>
<p>But, I don’t buy this argument because (1) the limited bandwidth of a single engine</p>
<p>simply indicates that multiple engines are needed and; (1) the 1 $mm^2$ area claim</p>
<p>is based on stale data from 2010 that showed such an engine can be implemented</p>
<p>on 45 nm technology, which, surely, can shrink significantly on today’s 3nm.</p>
<p>Moreover, the authors also propose quite intrusive CPU changes</p>
<p>### Applicable only to SGX</p>
<p>Versions and Merkle tree based memory protection memory are cool techniques,</p>
<p>but it seems that industry is moving away from these due to their prohibitively</p>
<p>high costs. SGX is the only system that used these.</p>
<p>But, SGX is being replaced by a new generation of VM based</p>
<p>TEEs (Intel TDX, AMD SEV, ARM CCA) that do not use versions or Merkle trees in memory</p>
<p>protection. Furthermore, SGX is being deprecated.</p>
<p>The authors should acknowledge that only SGX is in scope</p>
<p>and that other newer TEE technologies exist, which do not suffer from the</p>
<p>problem addressed by this submission.</p>
<p>#### Narrower scope: LLMs using SGX</p>
<p>Even though running machine learning, including LLMs, inside TEEs is gaining traction.</p>
<p>At the moment, it remains a niche that for now is mainly available through Azure [1].</p>
<p>Moreover, LLMs typically require numerous GPUs working together to train the model.</p>
<p>This implies the need for networking and GPU to GPU communication that should be discussed.</p>
<p>Additionally, this submission is focused on the case where the CPU and NPU collaborate</p>
<p>to train the model and it is unclear how common that is. Namely, the ZeRo-Offload [2]</p>
<p>infrastructure is only one way of training LLMs.</p>
<p>[1] <a target="_blank" rel="noopener" href="https://techcommunity.microsoft.com/t5/azure-confidential-computing/unlocking-the-potential-of-privacy-preserving-ai-with-azure/ba-p/3776838">Azure GPU confidential compute</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/atc21-ren-jie.pdf">ZeRO-Offload</a></p>
<p>### Miscellaneous</p>
<p>- In contrast to what is claimed: </p>
<p> &gt; integrated NPU TEE’s performance is limited due to power and area</p>
<p> &gt; constraints, which makes it insufficient for demanding applications such</p>
<p> &gt; as LLM training that involve large computations</p>
<p> Integrated NPUs can work just fine for example, see NVIDIA’s Grace-Hopper.</p>
<p>- Some citations need to be fixed:</p>
<p> &gt; [58] Tobin South, Guy Zuskind, Robert Mahari, and Thomas Hardjono. Secure</p>
<p> &gt; community transformers: Private pooled data for llms.</p>
<p> Is this a whitepaper? Add a link here and elsewhere and&#x2F;or describe where it was</p>
<p> published so it is easier to find.</p>
<p> &gt; Erhu Feng, Xu Lu, Dong Du, Bicheng Yang, Xueqiang Jiang, Yubin Xia, Binyu</p>
<p> &gt; Zang, and Haibo Chen. Scalable memory protection in the {PENGLAI} enclave.</p>
<p> &gt; In 15th {USENIX} Symposium on Operating Systems Design and Implementation</p>
<p> &gt; ({OSDI} 21), pages 275–294, 2021.</p>
<p> Remove the unnecessary {} here and elsewhere</p>
<p>Review #62D</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<p>Overall merit</p>
<p>-————</p>
<p>\2. Weak reject</p>
<p>Reviewer expertise</p>
<p>-—————–</p>
<p>\2. Some familiarity</p>
<p>Paper summary</p>
<p>-————</p>
<p>This paper targets CPU&#x2F;NPU collaborative ML training, for which both have disjoint memory spaces and are protected with respective TEEs. Under this setting, the data move between the CPU and NPU enclaves can become an overhead. The paper identifies a mismatch between CPU’s cacheline granularity and NPU’s cacheline granularity MAC management. The key idea is therefore a unified tensor-level granularity. The authors further propose delay verification and direct data transfer that optimize the data transfer.</p>
<p>Strengths</p>
<p>-——–</p>
<p>ML with S&amp;P concerns is an interesting design space.</p>
<p>Weaknesses</p>
<p>-———</p>
<p>Target a very specific hardware setting, which narrows the contribution.</p>
<p>Key ideas somewhat incremental </p>
<p>Would be nice to see evaluation on real hardware.</p>
<p>Detailed feedback</p>
<p>-—————-</p>
<p>I appreciate the paper’s fresh perspective towards ML by bringing the data security into consideration. Yet, the system model feels very specific, depending on a number of assumptions: ML training, CPU&#x2F;GPU collaboration, and a discrete NPU card. Removal of any assumption seems to make the problem go away (e.g. NPU only ML, or integrated CPU&#x2F;NPU). I cannot see such a scenario does not exist (someone wants to train models using Google’s cloud TPUs while not trusting Google for data security?), but it is certainly quite far from the mainstream practice for model training. </p>
<p>The key ideas, while seeming reasonable, feel incremental. It is basically cacheline granularity mismatch and some microscopic optimization for data move. Sorry if I sound too negative, but I could not find some high-level principles that can be taken away. </p>
<p>If the CPU TEE is designed with tensor granularity, does it lose generality in supporting other non-ML applications? How much would the overhead be?</p>
<p>Review #62E</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<p>Overall merit</p>
<p>-————</p>
<p>\3. Weak accept</p>
<p>Reviewer expertise</p>
<p>-—————–</p>
<p>\3. Knowledgeable</p>
<p>Paper summary</p>
<p>-————</p>
<p>This paper addresses the slowdown that emerges from a virtual TEE being </p>
<p>constructed using a CPU side TEE, that operates on cache line granularity</p>
<p>and pays for fine-grained authenticated encryption, and an NPU side TEE</p>
<p>that operates on tensor granularity – transferring the data from CPU</p>
<p>to NPU adds an overhead of 55%. </p>
<p>The authors propose tensor-granularity</p>
<p>metadata (VN, MAC) management in the CPU to align with the NPU and the </p>
<p>models, simplifies the data transfer between the CPU-NPU to only the </p>
<p>tensor VN and MAC needing to go over a trusted channel, and uses a delayed</p>
<p>MAC computation to check for tensor authenticity so that NPU can proceed</p>
<p>while the CPU has the metadata.</p>
<p>Together, the optimizations bring the overhead of a TEE down to a few </p>
<p>percent slowdown compared to non-secure training.</p>
<p>Strengths</p>
<p>-——–</p>
<p>The paper has a simple idea, to align and increase the granularity of</p>
<p>secure offchip memory to a tensor instead of a cache line level. But </p>
<p>it chases down the implications of this change well and proposes reasonable</p>
<p>edits to TEE construction.</p>
<p>Weaknesses</p>
<p>-———</p>
<p>The experimental setup is a custom construction with closed source cycle accurate</p>
<p>models for the NPU tied to RAMulator connected to a modified GEM5 etc – it would be</p>
<p>great to know how you check for correctness of the timing model.</p>
<p>The tradeoffs vs SoftVN could be explained more clearly.</p>
<p>Detailed feedback</p>
<p>-—————-</p>
<p>Can you please describe why SoftVN’s approach does not scale to a heterogeneous</p>
<p>TEE?</p>
<p>At what scale of algorithms would the proposed on-chip only metadata structures</p>
<p>be insufficient? What needs to be true of the models and training algorithms</p>
<p>for the proposed design to continue to apply?</p>
<p>SGX based TEEs have received complaints for not handling side-, covert-, and </p>
<p>speculative channels – what is the scope of these channels in the proposed</p>
<p>design (e.g., with delayed integrity checks).</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zhengxinyao.github.io/2024/02/07/About-me/" data-id="clt1brpzw0001wouxgupncq07" data-title="" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/02/25/post1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          post1
        
      </div>
    </a>
  
  
    <a href="/2024/02/07/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/02/25/post1/">post1</a>
          </li>
        
          <li>
            <a href="/2024/02/07/About-me/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/02/07/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ICT-IPRC<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>