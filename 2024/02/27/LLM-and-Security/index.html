<h1 id="LLM-and-Security"><a href="#LLM-and-Security" class="headerlink" title="LLM and Security"></a>LLM and Security</h1><h1 id="一、LLMs-for-Security"><a href="#一、LLMs-for-Security" class="headerlink" title="一、LLMs for Security"></a>一、LLMs for Security</h1><blockquote>
<p> <a href="https://eprint.iacr.org/2023/212">Generating secure hardware using chatgpt resistant to cwes</a><font color='red'>采用策略引导ChatGPT提供安全的硬件代码生成</font> 2023</p>
</blockquote>
<p>评估了ChatGPT平台上代码生成过程的安全性，特别是在硬件领域。探索了设计者可以采用的策略，使ChatGPT能够提供安全的硬件代码生成.</p>
<p>研究了10个值得注意的CWEs，并设计了技术来设计ChatGPT提示，使得生成的安全硬件能够抵抗所列出的CWEs。首先演示了设计人员为了生成硬件代码而发出的未经仔细检查的ChatGPT提示如何导致生成的代码中存在安全漏洞。然后系统地研究了设计人员为使ChatGPT能够推荐安全的硬件代码生成而采取的必要策略。</p>
<blockquote>
<p><a href="https://doi.org/10.48550/arXiv.2302.01215">Fixing hardware security bugs with large language models</a> <font color='red'>自动识别并修复硬件设计的安全相关bug</font>2023</p>
</blockquote>
<p>将关注点转移到硬件安全上。研究了LLMs，特别是OpenAI的Codex，在自动识别和修复硬件设计中与安全相关的bug方面的使用。</p>
<p>重点研究了硬件描述语言Verilog编写的代码中的缺陷修复。建立了一个具有领域代表性的硬件安全缺陷语料库。然后设计并实现了一个框架来定量评估负责修复指定bug的LLM的性能。该框架支持对提示词(即,快速工程)的设计空间探索，并识别LLM的最佳参数。我们证明了一个LLMs的集合可以修复我们所有的十个基准。在其自身的缺陷集合上优于最先进的Cirfix硬件缺陷修复工具。这些结果表明，LLMs可以修复硬件安全缺陷，并且该框架是迈向自动化端到端缺陷修复框架最终目标的重要一步。</p>
<p>我们对未来的工作提出了以下方向：</p>
<ul>
<li><p>测试一种针对安全相关缺陷的混合方法。使用、形式化验证、模糊测试、错误定位和静态分析工具进行检测，使用LLMs、Oracle指导的修改算法进行修复。这些选项的集合可能比单独使用一种技术更成功</p>
</li>
<li><p>在HDL上微调LLM，看性能是否提高。这提高了功能码的生成</p>
<p>【<a href="https://ieeexplore.ieee.org/abstract/document/10137086">Benchmarking Large Language Models for Automated Verilog RTL Code Generation</a> </p>
<p><font color='red'>在verilog库上微调LLM，并测试生成的代码的正确性</font>从 GitHub 和 Verilog 教科书中收集的 Verilog 数据集，在此数据集上微调预训练的 LLM。构建了一个评估框架，包括用于功能分析的测试平台和一个流程，用于测试为响应不同难度的问题而生成的Verilog代码的语法。】</p>
</li>
<li><p>探索使用参数全扫描的LLMs对功能缺陷的修复。我们只使用了在我们的实验中表现最好的一组参数。</p>
</li>
</ul>
<blockquote>
<p><a href="https://eprint.iacr.org/2023/606">Novel approach to cryptography implementation using chatgpt</a> <font color='red'>ChatGPT生成密码学代码</font>2023</p>
</blockquote>
<p>使用ChatGPT实现密码学，最终保护数据机密性。尽管缺乏广泛的编码技巧或编程知识，但作者能够通过ChatGPT成功地实现密码算法。这凸显了个体利用ChatGPT进行密码学任务的潜力。</p>
<p>实验表明，实现众所周知的算法(AES,CHAM,) 非常简单，ChatGPT根据需求精确生成源代码。生成的源代码无误码编译，并与测试向量精确匹配。对于未知的算法则比较困难，需要向ChatGPT发送了额外的修复请求。尽管如此，该过程仍然比从头实现分组密码更易于管理。具体来说，设计模块化的函数调用是开发人员需要做的事情，而ChatGPT可以快速生成准确的函数调用位置和命名约定。</p>
<p>从本质上讲，ChatGPT可以为相当一部分的密码实现提供便利。但是，必须仔细分析ChatGPT生成的代码，以识别和纠正任何错误。虽然ChatGPT可以被要求修复错误，但程序员往往可以更快更准确地做出必要的调整。</p>
<blockquote>
<p><a href="https://repository.lib.ncsu.edu/server/api/core/bitstreams/dfda7c06-04cb-425a-b0c9-2c699bf42e77/content">Agentsca: Advanced physical side channel analysis agent with llms</a><font color='red'>LLM技术开发侧信道分析</font>2023</p>
</blockquote>
<p>应用LLM技术来开发侧信道分析方法。该研究包括3种不同的方法：提示工程、微调LLM和基于人类反馈强化学习的微调LLM。本研究的主要目的是比较这三种方法在两种不同的侧信道分析场景中的性能：AES侧信道分析和深度学习加速器侧信道分析。</p>
<p>第一种方法，即提示工程，涉及设计特定的提示，以引导模型理解侧信道信息并提高其分析能力。该方法利用关于侧信道分析的先验知识和专长来显式地编码指令。第二种方法是微调LLM，其重点是使用预训练的语言模型，并进一步使用侧信道分析数据集进行微调。该方法无需人工提示工程，并允许模型从真实世界场景中学习。第三种方法是结合人类反馈强化学习的LLM微调方法，将人类反馈纳入微调过程。该方法旨在弥合人类专业知识与机器学习模型之间的鸿沟。</p>
<p>研究表明，LLMs在基于知识的任务中表现优异，但在生成性任务中表现不佳。在Agent SCA的情况下，单独的微调会导致模型对特定条件的过拟合，使其无法在所需的限制内生成独特的结构，并且容易产生幻觉。为了解决这些局限性，探索了将强化学习与人类反馈相结合的方法。人为引导在保证系统的效率和准确性方面至关重要。即时工程通过实现AgentSCA的少量提示转向，初步显示了良好的效果。然而，从长远来看，这种方法被发现是低效的，因为它需要从大量可能的解决方案中获得单一的响应。</p>
<p>目前，整个系统还无法成功攻击Edge TPU机器学习加速器。然而，它具有生成适用于谷歌TPU架构的功耗模型的潜力。这些模型尚未完全发挥作用，但仍有进一步改进的空间。另一方面，当涉及到生成AES攻击时，系统能够成功生成并详细阐述攻击。这证明了分析与AES加密相关的侧信道信息并利用其进行成功攻击方面的有效性。</p>
<h1 id="二、Privacy-Protected-LLMs"><a href="#二、Privacy-Protected-LLMs" class="headerlink" title="二、Privacy Protected LLMs"></a>二、Privacy Protected LLMs</h1><p>通过最先进的隐私增强技术(例如,零知识证明 ,差分隐私[ 233,175,159 ]和联邦学习[ 140,117,77 ] )来增强LLM</p>
<ul>
<li>“Privacy and data protection in chatgpt and other ai chatbots: Strategies for securing user information,”</li>
<li>“Differentially private decoding in large language models,”</li>
<li>“Privacy-preserving prompt tuning for large language model services,”</li>
<li>“Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning,”</li>
<li>“Chatgpt passing usmle shines a spotlight on the flaws of medical education,”</li>
<li>“Fate-llm: A industrial grade federated learning framework for large language models,”</li>
</ul>
<h1 id="三、Attacks-on-LLMs"><a href="#三、Attacks-on-LLMs" class="headerlink" title="三、Attacks on LLMs"></a>三、Attacks on LLMs</h1><h2 id="侧信道攻击"><a href="#侧信道攻击" class="headerlink" title="侧信道攻击"></a>侧信道攻击</h2><p>“Privacy side channels in machine learning systems,”引入了隐私侧信道攻击，这是一种利用系统级组件(例如,数据过滤、输出监控等)以远高于单机模型所能实现的速度提取隐私信息的攻击。提出了覆盖整个ML生命周期的4类侧信道，实现了增强型成员推断攻击和新型威胁(例如,提取用户的测试查询)</p>
<h2 id="数据中毒攻击"><a href="#数据中毒攻击" class="headerlink" title="数据中毒攻击"></a>数据中毒攻击</h2><ul>
<li>“Universal jailbreak backdoors from poisoned human feedback,”</li>
<li>“On the exploitability of instruction tuning,”</li>
<li>“Promptspecific poisoning attacks on text-to-image generative models,”</li>
<li>“Poisoning language models during instruction tuning,”</li>
</ul>
<h2 id="后门攻击"><a href="#后门攻击" class="headerlink" title="后门攻击"></a>后门攻击</h2><ul>
<li>“Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger,”</li>
<li>“Large language models are better adversaries: Exploring generative clean-label backdoor attacks against text classifiers,”</li>
<li>“Poisonprompt: Backdoor attack on prompt-based large language models,”</li>
</ul>
<h2 id="属性推断攻击"><a href="#属性推断攻击" class="headerlink" title="属性推断攻击"></a>属性推断攻击</h2><ul>
<li>“Beyond memorization: Violating privacy via inference with large language models,”首次全面考察了预训练的LLMs从文本中推断个人信息的能力</li>
</ul>
<h2 id="提取训练数据"><a href="#提取训练数据" class="headerlink" title="提取训练数据"></a>提取训练数据</h2><ul>
<li>“Ethicist: Targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation,”</li>
<li>“Canary extraction in natural language understanding models,”</li>
<li>“What do code models memorize? an empirical study on large language models of code,”</li>
<li>“Are large pre-trained language models leaking your personal information?”</li>
<li>“Text revealer: Private text reconstruction via model inversion attacks against transformers,”</li>
</ul>
<h2 id="提取模型"><a href="#提取模型" class="headerlink" title="提取模型"></a>提取模型</h2><ul>
<li>“Data-free model extraction,”</li>
</ul>
<h1 id="四、Defences-on-LLMs"><a href="#四、Defences-on-LLMs" class="headerlink" title="四、Defences on LLMs"></a>四、Defences on LLMs</h1><h2 id="模型架构防御"><a href="#模型架构防御" class="headerlink" title="模型架构防御"></a>模型架构防御</h2><ul>
<li>“Large language models can be strong differentially private learners,”具有较大参数规模的语言模型可以更有效地以差分隐私的方式进行训练。</li>
<li>“Promptbench: Towards evaluating the robustness of large language models on adversarial prompts,”</li>
<li>“Evaluating the instructionfollowing robustness of large language models to prompt injection,”更广泛的参数规模的LLMs，通常表现出对对抗攻击更高的鲁棒性。</li>
<li>“Revisiting out-of-distribution robustness in nlp: Benchmark, analysis, and llms evaluations,”在Out - of - distribution ( OOD )鲁棒性场景中也验证了这一点</li>
<li>“Synergistic integration of large language models and cognitive architectures for robust ai: An exploratory analysis,”通过将多种认知架构融入LLM来提高人工智能的鲁棒性。</li>
<li>“Building trust in conversational ai: A comprehensive review and solution architecture for explainable, privacy-aware systems using llms and knowledge graph,”与外部模块（知识图谱）相结合来提高LLM的安全性</li>
<li></li>
</ul>
<h2 id="LLM训练的防御：对抗训练"><a href="#LLM训练的防御：对抗训练" class="headerlink" title="LLM训练的防御：对抗训练"></a>LLM训练的防御：对抗训练</h2><ul>
<li>“Adversarial training for large neural language models,”</li>
<li>“Improving neural language modeling via adversarial training,”</li>
<li>“Freelb: Enhanced adversarial training for natural language understanding,”</li>
<li>“Towards improving adversarial training of nlp models,”</li>
<li>“Token-aware virtual adversarial training in natural language understanding,”</li>
<li>“Towards deep learning models resistant to adversarial attacks,”</li>
<li>“Achieving model robustness through discrete adversarial training,”</li>
<li>“Towards improving adversarial training of nlp models,”</li>
<li>“Improving neural language modeling via adversarial training,”</li>
<li>“Adversarial training for large neural language models,”</li>
<li>“Freelb: Enhanced adversarial training for natural language understanding,”</li>
<li>“Token-aware virtual adversarial training in natural language understanding,”</li>
</ul>
<h2 id="LLM训练的防御：鲁棒微调"><a href="#LLM训练的防御：鲁棒微调" class="headerlink" title="LLM训练的防御：鲁棒微调"></a>LLM训练的防御：鲁棒微调</h2><ul>
<li>“How should pretrained language models be fine-tuned towards adversarial robustness?”</li>
<li>“Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization,”</li>
<li>“Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions,”</li>
</ul>
<h2 id="LLM推理的防御：指令预处理"><a href="#LLM推理的防御：指令预处理" class="headerlink" title="LLM推理的防御：指令预处理"></a>LLM推理的防御：指令预处理</h2><ul>
<li>“Baseline defenses for adversarial attacks against aligned language models,”评估了多种针对越狱攻击的基线预处理方法，包括重令牌化和复述。</li>
<li>“On the reliability of watermarks for large language models,”评估了多种针对越狱攻击的基线预处理方法，包括重令牌化和复述</li>
<li>“Text adversarial purification as defense against adversarial attacks,”通过先对输入令牌进行掩码，然后与其他LLMs一起预测被掩码的令牌来净化指令。</li>
<li>“Jailbreak and guard aligned language models with only few in-context demonstrations,”证明了在指令中插入预定义的防御性证明可以有效地防御LLMs的越狱攻击。</li>
<li>“Testtime backdoor mitigation for black-box large language models with defensive demonstrations,”证明了在指令中插入预定义的防御性证明可以有效地防御LLMs的越狱攻击。</li>
</ul>
<h2 id="LLM推理的防御：恶意检测"><a href="#LLM推理的防御：恶意检测" class="headerlink" title="LLM推理的防御：恶意检测"></a>LLM推理的防御：恶意检测</h2><p>提供了对LLM中间结果的深度检查，如神经元激活</p>
<ul>
<li>“Defending against backdoor attacks in natural language generation,”提出用后向概率检测后门指令。</li>
<li>“A survey on evaluation of large language models,”从掩蔽敏感性的角度区分了正常指令和中毒指令。</li>
<li>“Bddr: An effective defense against textual backdoor attacks,”根据可疑词的文本相关性来识别可疑词。</li>
<li>“Rmlm: A flexible defense framework for proactively mitigating word-level adversarial attacks,”根据多代之间的语义一致性来检测对抗样本</li>
<li>“Shifting attention to relevance: Towards the uncertainty estimation of large language models,”在LLMs的不确定性量化中对此进行了探索</li>
<li>“Onion: A simple and effective defense against textual backdoor attacks,”利用了语言统计特性，例如检测孤立词。</li>
</ul>
<h2 id="LLM推理的防御：生成后处理"><a href="#LLM推理的防御：生成后处理" class="headerlink" title="LLM推理的防御：生成后处理"></a>LLM推理的防御：生成后处理</h2><ul>
<li>“Jailbreaker in jail: Moving target defense for large language models,”通过与多个模型候选物比较来减轻生成的毒性。</li>
<li>“Llm self defense: By self examination, llms know they are being tricked,”</li>
</ul>
<h1 id="五、LLMs-and-Hardware"><a href="#五、LLMs-and-Hardware" class="headerlink" title="五、LLMs and Hardware"></a>五、LLMs and Hardware</h1>